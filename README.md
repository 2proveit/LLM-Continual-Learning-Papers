# LLM-Continual-Learning-Papers

[![Awesome](https://camo.githubusercontent.com/64f8905651212a80869afbecbf0a9c52a5d1e70beab750dea40a994fa9a9f3c6/68747470733a2f2f617765736f6d652e72652f62616467652e737667)](https://github.com/AGI-Edgerunners/LLM-Continual-Learning-Papers) [![License: MIT](https://camo.githubusercontent.com/fd551ba4b042d89480347a0e74e31af63b356b2cac1116c7b80038f41b04a581/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d677265656e2e737667)](https://opensource.org/licenses/MIT) <img src="https://img.shields.io/github/last-commit/tensorflow/tensorflow.svg"/> [![img](https://camo.githubusercontent.com/eafac29b763e18c4d80c680d6a179f348cfa2afbc8d3a45642df19fd580d2404/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5052732d57656c636f6d652d726564)](https://camo.githubusercontent.com/eafac29b763e18c4d80c680d6a179f348cfa2afbc8d3a45642df19fd580d2404/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5052732d57656c636f6d652d726564)

Must-read Papers on Large Language Model (LLM) Continual Learning

----

1. **Towards Continual Knowledge Learning of Language Models**

   *Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, Stanley Jungkyu Choi, Minjoon Seo.* [[abs](https://arxiv.org/abs/2110.03215)]. ICLR 2022.

1. **Continual Pre-Training Mitigates Forgetting in Language and Vision**

   *Andrea Cossu, Tinne Tuytelaars, Antonio Carta, Lucia Passaro, Vincenzo Lomonaco, Davide Bacci.* [[abs](https://arxiv.org/abs/2205.09357)]. Preprint 2022.05.

1. **Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora**

   *Xisen Jin, Dejiao Zhang, Henghui Zhu, Wei Xiao, Shang-Wen Li, Xiaokai Wei, Andrew Arnold, Xiang Ren.* [[abs](https://arxiv.org/abs/2110.08534)]. NAACL 2022

1. **Continual Training of Language Models for Few-Shot Learning**

   *Zixuan Ke, Haowei Lin, Yijia Shao, Hu Xu, Lei Shu, Bing Liu.* [[abs](https://arxiv.org/abs/2210.05549)]. EMNLP 2022.

1. **Continual Pre-training of Language Models**

   *Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, Bing Liu.* [[abs](https://arxiv.org/abs/2302.03241)]. ICLR 2023.

1. **Progressive Prompts: Continual Learning for Language Models**

   *Anastasia Razdaibiedina, Yuning Mao, Rui Hou, Madian Khabsa, Mike Lewis, Amjad Almahairi.* [[abs](https://arxiv.org/abs/2301.12314)]. ICLR 2023.

1. **A Unified Continual Learning Framework with General Parameter-Efficient Tuning**

   *Qiankun Gao, Chen Zhao, Yifan Sun, Teng Xi, Gang Zhang, Bernard Ghanem, Jian Zhang.* [[abs](https://arxiv.org/abs/2303.10070)]. ICCV 2023.
  
1. **Semiparametric Language Models Are Scalable Continual Learners**

   *Guangyue Peng, Tao Ge, Si-Qing Chen, Furu Wei, Houfeng Wang.* [[abs](https://arxiv.org/abs/2303.01421)]. Preprint 2023.02.

1. **Continual Pre-Training of Large Language Models: How to (re)warm your model?**

   *Kshitij Gupta, Benjamin Thérien, Adam Ibrahim, Mats L. Richter, Quentin Anthony, Eugene Belilovsky, Irina Rish, Timothée Lesort.* [[abs](https://arxiv.org/abs/2308.04014)]. ICML 2023 Workshop.

1. **ConPET: Continual Parameter-Efficient Tuning for Large Language Models**

   *Chenyang Song, Xu Han, Zheni Zeng, Kuai Li, Chen Chen, Zhiyuan Liu, Maosong Sun, Tao Yang.* [[abs](https://arxiv.org/abs/2309.14763)]. Preprint 2023.09.

1. **TRACE: A Comprehensive Benchmark for Continual Learning in Large Language Models**

   *Xiao Wang, Yuansen Zhang, Tianze Chen, Songyang Gao, Senjie Jin, Xianjun Yang, Zhiheng Xi, Rui Zheng, Yicheng Zou, Tao Gui, Qi Zhang, Xuanjing Huang.* [[abs](https://arxiv.org/abs/2310.06762)]. Preprint 2023.10.

1. **A Study of Continual Learning Under Language Shift**

   *Evangelia Gogoulou, Timothée Lesort, Magnus Boman, Joakim Nivre.* [[abs](https://arxiv.org/abs/2311.01200)]. Preprint 2023.11.

1. **Scalable Language Model with Generalized Continual Learning**

   *ICLR 2024 Conference Submission1284 Authors.* [[openreview](https://openreview.net/forum?id=mz8owj4DXu)]. Preprint 2023.

1. **Efficient Continual Pre-training for Building Domain Specific Large Language Models**

   *ICLR 2024 Conference Submission4091 Authors.* [[openreview](https://openreview.net/forum?id=onyGT5Nbuz)]. Preprint 2023.
