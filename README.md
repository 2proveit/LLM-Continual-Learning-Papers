# LLM-Continual-Learning-Papers
Must-read Papers on Large Language Model (LLMs) Continual Learning

1. **Towards Continual Knowledge Learning of Language Models**

   *Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, Stanley Jungkyu Choi, Minjoon Seo.* [[abs](https://arxiv.org/abs/2110.03215)]. ICLR 2022.

1. **Semiparametric Language Models Are Scalable Continual Learners**

   *Guangyue Peng, Tao Ge, Si-Qing Chen, Furu Wei, Houfeng Wang.* [[abs](https://arxiv.org/abs/2303.01421)]. Preprint 2023.02.

1. **Continual Pre-training of Language Models**

   *Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, Bing Liu.* [[abs](https://arxiv.org/abs/2302.03241)]. ICLR 2023.

