# LLM-Continual-Learning-Papers

[![Awesome](https://camo.githubusercontent.com/64f8905651212a80869afbecbf0a9c52a5d1e70beab750dea40a994fa9a9f3c6/68747470733a2f2f617765736f6d652e72652f62616467652e737667)](https://github.com/AGI-Edgerunners/LLM-Continual-Learning-Papers) [![License: MIT](https://camo.githubusercontent.com/fd551ba4b042d89480347a0e74e31af63b356b2cac1116c7b80038f41b04a581/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d677265656e2e737667)](https://opensource.org/licenses/MIT) <img src="https://img.shields.io/github/last-commit/tensorflow/tensorflow.svg"/> [![img](https://camo.githubusercontent.com/eafac29b763e18c4d80c680d6a179f348cfa2afbc8d3a45642df19fd580d2404/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5052732d57656c636f6d652d726564)](https://camo.githubusercontent.com/eafac29b763e18c4d80c680d6a179f348cfa2afbc8d3a45642df19fd580d2404/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5052732d57656c636f6d652d726564)

Must-read Papers on Large Language Model (LLMs) Continual Learning

--

1. **Towards Continual Knowledge Learning of Language Models**

   *Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, Stanley Jungkyu Choi, Minjoon Seo.* [[abs](https://arxiv.org/abs/2110.03215)]. ICLR 2022.

1. **Semiparametric Language Models Are Scalable Continual Learners**

   *Guangyue Peng, Tao Ge, Si-Qing Chen, Furu Wei, Houfeng Wang.* [[abs](https://arxiv.org/abs/2303.01421)]. Preprint 2023.02.

1. **Continual Pre-training of Language Models**

   *Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, Bing Liu.* [[abs](https://arxiv.org/abs/2302.03241)]. ICLR 2023.

